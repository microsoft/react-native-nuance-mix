//  Copyright (C) 2020 - 2021 Nuance Communications, Inc. All Rights Reserved.
//
//  The copyright to the computer program(s) herein is the property of
//  Nuance Communications, Inc. The program(s) may be used and/or copied
//  only with the written permission from Nuance Communications, Inc.
//  or in accordance with the terms and conditions stipulated in the
//  agreement/contract under which the program(s) have been supplied.
syntax = "proto3";
package nuance.dlg.v1;

option java_multiple_files = true;
option java_package = "com.nuance.coretech.dialog.v1.service.messages";

import "nuance/asr/v1/recognizer.proto";
import "nuance/asr/v1/result.proto";
import "nuance/asr/v1/resource.proto";
import "nuance/tts/v1/nuance_tts_v1.proto";
import "nuance/dlg/v1/common/dlg_common_messages.proto";

// Performs recognition on streamed audio using ASRaaS and requests speech synthesis using TTSaaS.
message StreamInput {
  ExecuteRequest request = 1; //Standard DLGaaS ExecuteRequest; used to continue the dialog interactions.
  oneof asr_control_oneof {
    AsrParamsV1 asr_control_v1 = 2; //Parameters to be forwarded to the ASR service.
  }
  bytes audio = 3;                //Audio samples in the selected encoding for recognition.
  oneof tts_control_oneof {
    TtsParamsV1 tts_control_v1 = 4; //Parameters to be forwarded to the TTS service.
  }
  nuance.asr.v1.ControlMessage control_message = 5; // Optional second message in the input stream, for ASR timer control.
}

//Streams the requested TTS output and returns ASR results.
message StreamOutput {
  ExecuteResponse response = 1;                   //Standard DLGaaS ExecuteResponse; used to continue the dialog interactions.
  nuance.tts.v1.SynthesisResponse audio = 2; //TTS output.
  nuance.asr.v1.Result asr_result = 3; //Output message containing the transcription result, including the result type, the start and end times, metadata about the transcription, and one or more transcription hypotheses. Included in [RecognizeResponse](#recognizeresponse).
  nuance.asr.v1.Status asr_status = 4; //Output message indicating the status of the transcription. 
  nuance.asr.v1.StartOfSpeech asr_start_of_speech = 5; //Output message containing the start-of-speech message.
}

//Request object used by the Start method.
message StartRequest {
  string session_id = 1; //Optional session ID. If not provided then one will be generated.
  nuance.dlg.v1.common.Selector selector = 2; //Selector providing the channel and language used for the conversation.
  nuance.dlg.v1.common.StartRequestPayload payload = 3; // Payload of the Start request.
  uint32 session_timeout_sec = 4; //Session timeout value (in seconds), after which the session is terminated. The maximum is configured in the deployment.
  string user_id = 5; //Identifies a specific user within the application.
  map<string, string> client_data = 6; //Optional client-supplied key-value pairs to inject into the call log.
}

//Response object used by the Start method.
message StartResponse {
  nuance.dlg.v1.common.StartResponsePayload payload = 1; //Payload of the Start response.
}

//Request object used by the Update method.
message UpdateRequest {
  string session_id = 1; // ID for the session.
  nuance.dlg.v1.common.UpdateRequestPayload payload = 2; //Payload of the Update request.
  map<string, string> client_data = 3; //Optional client-supplied key-value pairs to inject into the call log.
  string user_id = 4; //Identifies a specific user within the application.
}

//Response object used by the Update method.
message UpdateResponse {
}

//Request object used by the Execute method.
message ExecuteRequest {
  string session_id = 1; // ID for the session.
  nuance.dlg.v1.common.Selector selector = 2; //Selector providing the channel and language used for the conversation.
  nuance.dlg.v1.common.ExecuteRequestPayload payload = 3; //Payload of the Execute request.
  string user_id = 5; //Identifies a specific user within the application. 
}

// Parameters to be forwarded to the ASR service.
message AsrParamsV1 {
  nuance.asr.v1.AudioFormat audio_format = 1; //Audio codec type and sample rate.
  nuance.asr.v1.EnumUtteranceDetectionMode utterance_detection_mode = 2; //How end of utterance is determined. Defaults to SINGLE.
  nuance.asr.v1.RecognitionFlags recognition_flags = 3; //Flags to fine tune recognition.
  nuance.asr.v1.EnumResultType result_type = 4; //Whether final, partial, or immutable results are returned.

  uint32 no_input_timeout_ms = 5;             // Maximum silence, in ms, allowed while waiting for user input after recognition timers are started. Default (0) means server default, usually no timeout.
  uint32 recognition_timeout_ms = 6;          // Maximum duration, in ms, of recognition turn. Default (0) means server default, usually no timeout.
  uint32 utterance_end_silence_ms = 7;        // Minimum silence, in ms, that determines the end of an utterance. Default (0) means server default, usually 500ms or half a second.
  oneof optional_speech_detection_sensitivity {
    float speech_detection_sensitivity = 8; // A balance between detecting speech and noise (breathing, etc.), 0 to 1. 0 means ignore all noise, 1 means interpret all noise as speech. Default is 0.5.
  }
  uint32 max_hypotheses = 9; // Maximum number of n-best hypotheses to return. Default (0) means a server default.

  bool end_stream_no_valid_hypotheses = 10;  // Determines whether the dialog or the client application handles the dialog flow when ASRaaS does not return a valid hypothesis. When set to false (default), the dialog flow is determined by the Mix.dialog application. Set to true to close the stream and hand the control back to the client application.
  repeated nuance.asr.v1.RecognitionResource resources = 11; // Repeated. Resources (DLMs, wordsets, builtins) to improve recognition.
  string speech_domain = 12;                  // Mapping to internal weight sets for language models in the data pack. Values depend on the data pack.
  nuance.asr.v1.Formatting formatting = 13;                 // Specifies how the transcription results are presented, using keywords for formatting schemes and options supported by the data pack. 
}

// Parameters to be forwarded to the TTS service. 
message TtsParamsV1 {
  nuance.tts.v1.AudioParameters audio_params = 1; //Output audio parameters, such as encoding and volume.
  nuance.tts.v1.Voice voice = 2;//The voice to use for audio synthesis.
}

//Response object used by the Execute method.
message ExecuteResponse {
  nuance.dlg.v1.common.ExecuteResponsePayload payload = 1; //Payload of the Execute response.
}

//Request object used by Stop method.
message StopRequest {
  string session_id = 1;  // ID for the session.
  string user_id = 5; //Identifies a specific user within the application. 
}

//Response object used by the Stop method. Currently empty; reserved for future use.
message StopResponse{
}

//Request object used by Status method.
message StatusRequest {
  string session_id = 1;  // ID for the session.
}

//Response object used by the Status method.
message StatusResponse{
  uint32 session_remaining_sec = 1; //Remaining session time to live (TTL) value in seconds, after which the session is terminated. Note: the TTL may be a few seconds off based on how long the round trip of the request took.
}